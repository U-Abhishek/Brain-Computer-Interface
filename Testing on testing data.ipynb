{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "Ls1_QPAmusUZ"
   },
   "outputs": [],
   "source": [
    "## analyse the data\n",
    "import mne\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 493
    },
    "id": "rNNd3ANCyKoF",
    "outputId": "d7fd2aeb-5d83-4cd3-edcc-20d52f330716",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting EDF parameters from C:\\Users\\uabhi\\NEW_MY_JUPYTER\\BCI\\BCI 4 dataset\\BCICIV_2a_gdf\\A01E.gdf...\n",
      "GDF file detected\n",
      "Setting channel info structure...\n",
      "Could not determine channel type of the following channels, they will be set as EEG:\n",
      "EEG-Fz, EEG, EEG, EEG, EEG, EEG, EEG, EEG-C3, EEG, EEG-Cz, EEG, EEG-C4, EEG, EEG, EEG, EEG, EEG, EEG, EEG, EEG-Pz, EEG, EEG\n",
      "Creating raw.info structure...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\uabhi\\anaconda3\\envs\\tf\\lib\\site-packages\\mne\\io\\edf\\edf.py:1123: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
      "  etmode = np.fromstring(etmode, UINT8).tolist()[0]\n",
      "C:\\Users\\uabhi\\anaconda3\\envs\\tf\\lib\\contextlib.py:120: RuntimeWarning: Channel names are not unique, found duplicates for: {'EEG'}. Applying running numbers for duplicates.\n",
      "  next(self.gen)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"table table-hover table-striped table-sm table-responsive small\">\n",
       "    <tr>\n",
       "        <th>Measurement date</th>\n",
       "        \n",
       "        <td>January 19, 2005  12:00:00 GMT</td>\n",
       "        \n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>Experimenter</th>\n",
       "        \n",
       "        <td>Unknown</td>\n",
       "        \n",
       "    </tr>\n",
       "        <th>Participant</th>\n",
       "        \n",
       "        <td>Unknown</td>\n",
       "        \n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>Digitized points</th>\n",
       "        \n",
       "        <td>0 points</td>\n",
       "        \n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>Good channels</th>\n",
       "        <td>22 EEG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>Bad channels</th>\n",
       "        <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>EOG channels</th>\n",
       "        <td>Not available</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>ECG channels</th>\n",
       "        <td>Not available</td>\n",
       "    \n",
       "    <tr>\n",
       "        <th>Sampling frequency</th>\n",
       "        <td>250.00 Hz</td>\n",
       "    </tr>\n",
       "    \n",
       "    \n",
       "    <tr>\n",
       "        <th>Highpass</th>\n",
       "        <td>0.50 Hz</td>\n",
       "    </tr>\n",
       "    \n",
       "    \n",
       "    <tr>\n",
       "        <th>Lowpass</th>\n",
       "        <td>100.00 Hz</td>\n",
       "    </tr>\n",
       "    \n",
       "    \n",
       "    \n",
       "    <tr>\n",
       "        <th>Filenames</th>\n",
       "        <td>A01E.gdf</td>\n",
       "    </tr>\n",
       "    \n",
       "    <tr>\n",
       "        <th>Duration</th>\n",
       "        <td>00:45:47 (HH:MM:SS)</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<RawGDF | A01E.gdf, 22 x 687000 (2748.0 s), ~26 kB, data not loaded>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw=mne.io.read_raw_gdf('BCICIV_2a_gdf/A01E.gdf',\n",
    "                         eog=['EOG-left', 'EOG-central', 'EOG-right'])\n",
    "raw.drop_channels(['EOG-left', 'EOG-central', 'EOG-right'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**From the paper**  \n",
    "'1023': 1 (Rejected trial),   \n",
    " '1072': 2 (Eye movements),  \n",
    " '276':  3 (eyes open)),  \n",
    " '277':  4 (eyes closed),   \n",
    " '32766':5 (Start of a new run),  \n",
    " '768':  6 (Start of a trial),  \n",
    " '769': 7 (class 1),  \n",
    " '770': 8 (class 2),  \n",
    " '771': 9 (class 3),  \n",
    " '772': 10 (class 4)\n",
    "#### event ids are not same for all subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "TitH-M4tCYx9"
   },
   "outputs": [],
   "source": [
    "def read_data(path):\n",
    "    raw=mne.io.read_raw_gdf(path,preload=True,\n",
    "                          eog=['EOG-left', 'EOG-central', 'EOG-right'])\n",
    "    raw.drop_channels(['EOG-left', 'EOG-central', 'EOG-right'])\n",
    "    raw.set_eeg_reference()\n",
    "    events, event_id =mne.events_from_annotations(raw)\n",
    "    #events[1]['769'],events[1]['770'],events[1]['771'],events[1]['772']\n",
    "    ann = event_id.keys()\n",
    "    ann = list(map(int,ann))\n",
    "    ids = event_id.values()\n",
    "    ids = list(ids)\n",
    "    unq = np.unique(events[:,-1])\n",
    "    for u in unq:\n",
    "        events[:,-1] = np.where(events[:,-1]== u , ann[u-1],events[:,-1])\n",
    "    epochs = mne.Epochs(raw, events, event_id=[769,772],\n",
    "                        tmin= 0, tmax=4,baseline=(None,4), on_missing ='warn')\n",
    "    labels=epochs.events[:,-1]\n",
    "    features=epochs.get_data()\n",
    "    return features,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BCICIV_2a_gdf\\\\A01T.gdf',\n",
       " 'BCICIV_2a_gdf\\\\A02T.gdf',\n",
       " 'BCICIV_2a_gdf\\\\A03T.gdf',\n",
       " 'BCICIV_2a_gdf\\\\A04T.gdf',\n",
       " 'BCICIV_2a_gdf\\\\A05T.gdf',\n",
       " 'BCICIV_2a_gdf\\\\A06T.gdf',\n",
       " 'BCICIV_2a_gdf\\\\A07T.gdf',\n",
       " 'BCICIV_2a_gdf\\\\A08T.gdf',\n",
       " 'BCICIV_2a_gdf\\\\A09T.gdf']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths = glob.glob('BCICIV_2a_gdf/*T.gdf')\n",
    "paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "xW0kIs4CEqlx"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "features,labels,groups=[],[],[]\n",
    "for i in range(len(paths)):\n",
    "    feature,label=read_data(paths[i])\n",
    "    features.append(feature)\n",
    "    labels.append(label)\n",
    "    groups.append([i+1]*len(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lVYPtkG9JW0V",
    "outputId": "4b94fbd7-7331-49c9-b9e9-f027dc3390d8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1296, 22, 1001), (1296,), (1296,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features=np.concatenate(features)\n",
    "labels=np.concatenate(labels)\n",
    "groups=np.concatenate(groups)\n",
    "\n",
    "features.shape,labels.shape,groups.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 9, 9, 9])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6CXhty7XLz3P",
    "outputId": "89d63f5d-50ad-4c51-a592-e7c006f0c898",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1, 2, 3, 4, 5, 6, 7, 8, 9]),\n",
       " array([144, 144, 144, 144, 144, 144, 144, 144, 144], dtype=int64))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique, counts = np.unique(groups, return_counts=True)\n",
    "unique, counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([769, 772]), array([648, 648], dtype=int64))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique, counts = np.unique(labels, return_counts=True)\n",
    "unique, counts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupKFold,LeaveOneGroupOut\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "gkf=GroupKFold()\n",
    "from sklearn.base import TransformerMixin,BaseEstimator\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#https://stackoverflow.com/questions/50125844/how-to-standard-scale-a-3d-matrix\n",
    "class StandardScaler3D(BaseEstimator,TransformerMixin):\n",
    "    #batch, sequence, channels\n",
    "    def __init__(self):\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    def fit(self,X,y=None):\n",
    "        self.scaler.fit(X.reshape(-1, X.shape[2]))\n",
    "        return self\n",
    "\n",
    "    def transform(self,X):\n",
    "        return self.scaler.transform(X.reshape( -1,X.shape[2])).reshape(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb = preprocessing.LabelBinarizer()\n",
    "\n",
    "#enc_df= enc.fit_transform(dataset['60'])\n",
    "labels =  lb.fit_transform(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_array = features\n",
    "label_array = labels\n",
    "group_array = groups\n",
    "data_array=np.moveaxis(data_array,1,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "accuracy=[]\n",
    "for train_index, val_index in gkf.split(data_array, label_array, groups=group_array):\n",
    "    train_features,train_labels=data_array[train_index],label_array[train_index]\n",
    "    val_features,val_labels=data_array[val_index],label_array[val_index]\n",
    "    scaler=StandardScaler3D()\n",
    "    train_features=scaler.fit_transform(train_features)\n",
    "    val_features=scaler.transform(val_features)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler=StandardScaler3D()\n",
    "train_features,val_features,train_labels,val_labels = train_test_split(data_array, label_array, test_size=0.2, random_state=42)\n",
    "a,b,train_gr,val_gr = train_test_split(data_array, group_array, test_size=0.2, random_state=42)\n",
    "train_features=scaler.fit_transform(train_features)\n",
    "val_features=scaler.transform(val_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1036, 1001, 22), (1036, 1))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features.shape, train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1036, 22, 1001), (1036, 22, 1001))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features=np.moveaxis(train_features,1,2)\n",
    "val_features =np.moveaxis(val_features,1,2)\n",
    "train_features.shape, train_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spitting test data person by person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(260,)\n",
      "(260, 22, 1001)\n",
      "(260, 1)\n",
      "[5 3 6 8 9 7 4 5 7 1 6 8 6 4 7 1 1 6 5 3 3 7 8 1 4 2 9 2 6 3 9 3 6 6 5 1 7\n",
      " 6 4 4 1 7 3 5 9 8 8 2 8 7 2 4 8 8 4 1 7 9 5 1 5 1 2 9 6 2 6 6 3 5 6 8 2 4\n",
      " 7 7 8 8 4 6 7 4 7 6 1 6 9 5 1 6 5 9 5 3 2 2 6 2 3 2 1 7 2 4 2 7 5 3 2 3 2\n",
      " 4 6 1 9 6 5 9 5 6 4 7 1 4 4 4 1 8 1 2 8 8 9 6 6 5 1 3 2 6 8 4 8 8 8 8 9 1\n",
      " 4 4 1 3 2 3 2 1 4 2 3 7 9 2 6 3 4 1 1 2 4 6 7 2 6 9 9 7 1 8 8 8 7 9 9 3 6\n",
      " 3 2 9 8 4 2 4 5 4 3 6 4 2 8 6 3 8 1 8 4 6 1 5 2 1 9 6 5 5 3 6 5 4 5 8 9 5\n",
      " 5 9 8 9 7 5 8 7 9 7 3 5 8 8 7 2 7 9 3 3 7 2 6 3 1 7 3 4 5 8 5 3 2 6 6 2 1\n",
      " 2]\n"
     ]
    }
   ],
   "source": [
    "print(val_gr.shape)\n",
    "print(val_features.shape)\n",
    "print(val_labels.shape)\n",
    "val_gr = np.array(val_gr)\n",
    "print(val_gr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1, 2, 3, 4, 5, 6, 7, 8, 9]),\n",
       " array([28, 32, 27, 29, 27, 35, 26, 32, 24], dtype=int64))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique, counts = np.unique(val_gr, return_counts=True)\n",
    "unique, counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_splitter(features, labels, groups):\n",
    "    testing_features = []\n",
    "    testing_labels = []\n",
    "    for i in range(1,10):\n",
    "        fe = []\n",
    "        la = []\n",
    "        for j in range(len(groups)):\n",
    "            if groups[j]==i:\n",
    "                fe.append(features[j])\n",
    "                la.append(labels[j])\n",
    "        testing_features.append(np.array(fe))\n",
    "        testing_labels.append(np.array(la))\n",
    "    return testing_features, testing_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2 3 4 5 6 7 8 9 "
     ]
    }
   ],
   "source": [
    "testing_features, testing_labels = group_splitter(val_features,val_labels,val_gr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(testing_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from EEGModels_binary_classifiers import *\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EegNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 22, 1001, 1)]     0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 22, 1001, 8)       512       \n",
      "                                                                 \n",
      " batch_normalization_9 (Batc  (None, 22, 1001, 8)      32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " depthwise_conv2d_3 (Depthwi  (None, 1, 1001, 16)      352       \n",
      " seConv2D)                                                       \n",
      "                                                                 \n",
      " batch_normalization_10 (Bat  (None, 1, 1001, 16)      64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_6 (Activation)   (None, 1, 1001, 16)       0         \n",
      "                                                                 \n",
      " average_pooling2d_6 (Averag  (None, 1, 250, 16)       0         \n",
      " ePooling2D)                                                     \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 1, 250, 16)        0         \n",
      "                                                                 \n",
      " separable_conv2d_3 (Separab  (None, 1, 250, 16)       512       \n",
      " leConv2D)                                                       \n",
      "                                                                 \n",
      " batch_normalization_11 (Bat  (None, 1, 250, 16)       64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_7 (Activation)   (None, 1, 250, 16)        0         \n",
      "                                                                 \n",
      " average_pooling2d_7 (Averag  (None, 1, 31, 16)        0         \n",
      " ePooling2D)                                                     \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 1, 31, 16)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 496)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 497       \n",
      "                                                                 \n",
      " softmax (Activation)        (None, 1)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,033\n",
      "Trainable params: 1,953\n",
      "Non-trainable params: 80\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model('Models/deepconvnet_LT_0.9038.h5')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(testing_features[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_accuracy(model,x,y):\n",
    "    accuracy = []\n",
    "    for i in range(len(x)):\n",
    "        y_p = []\n",
    "        pred = model.predict_on_batch(x[i])\n",
    "        pred = pred.reshape(pred.shape[0])\n",
    "        for l in pred:\n",
    "            if l>0.5:\n",
    "                y_p.append(1)\n",
    "            else:\n",
    "                y_p.append(0)\n",
    "        y_t = y[i].reshape(y[i].shape)\n",
    "        acc = accuracy_score(y_t, y_p)\n",
    "        accuracy.append(acc)\n",
    "    accuracy = np.array(accuracy)\n",
    "    return np.array(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = find_accuracy(model,testing_features,testing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.96428571, 0.8125    , 0.88888889, 0.86206897, 0.7037037 ,\n",
       "       0.82857143, 0.88461538, 0.90625   , 1.        ])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1, 2, 3, 4, 5, 6, 7, 8, 9]),\n",
       " array([28, 32, 27, 29, 27, 35, 26, 32, 24], dtype=int64))"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique, counts = np.unique(val_gr, return_counts=True)\n",
    "unique, counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8723204539535957"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "BCI Competition 2008 â€“ Graz data set A.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
